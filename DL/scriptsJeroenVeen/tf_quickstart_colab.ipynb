{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WS7zEMkQQu1P"},"source":["Load fashion MNIST, which is a drop-in replacement of MNIST. (70,000 grayscale images of 28 × 28 pixels each, with 10 classes)"]},{"cell_type":"code","metadata":{"id":"IMomRj2YLw4Q"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","fashion_mnist = keras.datasets.fashion_mnist\n","(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n","\n","# The class names are NOT included in the dataset, so\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","# Show some info\n","print(\"Tensorflow version: {:s}\".format(tf.__version__))\n","print(\"Keras version: {:s}\".format(keras.__version__))\n","print(\"X_train_full shape: {} and type: {}\".format(X_train_full.shape, X_train_full.dtype))\n","\n","# Scale pixel values down to the 0–1 range\n","# It's important that the training set and the testing set be preprocessed\n","X_train_full, X_test = X_train_full/255.0, X_test/255.0\n","\n","# Create a validation set\n","X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n","y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mA3aTGRARTL9"},"source":["Display the first 25 images from the training set and display the class name below each image"]},{"cell_type":"code","metadata":{"id":"3rbn76UhRamG"},"source":["plt.figure(figsize=(10,10))\n","for i in range(15):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(X_train[i], cmap=plt.cm.binary)\n","    plt.xlabel(class_names[y_train_full[i]])\n","plt.show(block=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apiV_U7TRtSu"},"source":["# Create a model\n","model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(28, 28)),\n","    keras.layers.Dense(300, activation=\"relu\"),\n","    keras.layers.Dense(100, activation='relu'),\n","    keras.layers.Dense(10, activation=\"softmax\")\n","])\n","\n","# Show some info\n","model.summary()\n","# Note that Dense layers often have a lot of parameters. For example, the first hidden\n","# layer has 28×28× density = 784 × 128 connection weights, plus 128 bias terms\n","# This gives the model quite a lot of flexibility to fit the training\n","# data, but it also means that the model runs the risk of overfitting, especially when you\n","# do not have a lot of training data.\n","\n","# Alternative way of constructing a model:\n","##model = keras.models.Sequential()\n","##model.add(keras.layers.Flatten(input_shape=[28, 28]))\n","##model.add(keras.layers.Dense(300, activation=\"relu\"))\n","##model.add(keras.layers.Dense(100, activation=\"relu\"))\n","##model.add(keras.layers.Dense(10, activation=\"softmax\"))\n","##\n","##Build the tf.keras.Sequential model by stacking layers. Choose an optimizer and loss function for training:\n","##\n","##model = tf.keras.models.Sequential([\n","##  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","##  tf.keras.layers.Dense(128, activation='relu'),\n","##  tf.keras.layers.Dropout(0.2),\n","##  tf.keras.layers.Dense(10)\n","##])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_SFV5RpYUz64"},"source":["Compile the model and shoose an optimizer.\n","We use the \"sparse_categorical_cross entropy\" loss because we have sparse labels\n","\n","When using the SGD optimizer, it is important to tune the learning\n","rate. So, you will generally want to use optimizer=keras.optimiz\n","ers.SGD(lr=???) to set the learning rate, rather than opti\n","mizer=\"sgd\", which defaults to lr=0.01."]},{"cell_type":"code","metadata":{"id":"sTXg_wxtRzel"},"source":["# Compile the model\n","model.compile(optimizer='adam', # 'sgd'\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","              metrics=['accuracy'])\n","\n","# Currently (dec 2020) it seems that cross-validation has not been implemented in Keras, so we would have to build something ourselves. No time for that right now\n","\n","# Feed the model\n","history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lElVx1NPT1Kr"},"source":["Show the learning curves"]},{"cell_type":"code","metadata":{"id":"AgKwF5WWT1rl"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","pd.DataFrame(history.history).plot(figsize=(8, 5))\n","plt.grid(True)\n","plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hg_HZzt4WxD7"},"source":["print(history.history.keys())\n","fig, ax = plt.subplots()\n","# Plot history: MAE\n","ax.plot(history.history['loss'], label='loss (training data)')\n","ax.plot(history.history['val_loss'], label='loss (validation data)')\n","ax.plot(history.history['accuracy'], label='accuracy (training data)')\n","ax.plot(history.history['val_accuracy'], label='accuracy (validation data)')\n","ax.set_title('Learning curves')\n","ax.set_ylabel('value')\n","ax.set_xlabel('No. epoch')\n","ax.grid(True)\n","ax.set_ylim(0,1)\n","ax.legend(loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5aISZuc5ULwn"},"source":["Check performance on the test set"]},{"cell_type":"code","metadata":{"id":"WxgfMSPTUNFv"},"source":["\n","test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n","print('\\nTest accuracy:', test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oIksY4pWLBS9"},"source":["Show some predictions"]},{"cell_type":"code","metadata":{"id":"Mk6xukLlMW0Q"},"source":["predictions = model.predict(X_test)\n","most_probable_predictions = np.argmax(predictions,axis=1)\n","\n","# Plot confusion matrix\n","from sklearn.metrics import confusion_matrix\n","\n","cm = np.round(confusion_matrix(y_test, most_probable_predictions, normalize='true'),1)\n","fig0, ax0 = plt.subplots(1,1)\n","ax0.imshow(cm, cmap=plt.cm.Blues)\n","ax0.set_xlabel(\"Predicted labels\")\n","ax0.set_ylabel(\"True labels\")\n","ax0.set_xticks(np.arange(len(class_names)))\n","ax0.set_yticks(np.arange(len(class_names)))\n","ax0.set_xticklabels(class_names, rotation = 45, ha=\"right\")\n","ax0.set_yticklabels(class_names)\n","ax0.set_title('Confusion matrix ')\n","# ax0.set_colorbar()\n","plt.tight_layout()\n","\n","import seaborn as sns\n","plt.figure()\n","ax4 = sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, xticklabels=class_names, yticklabels=class_names)\n","plt.tight_layout()"],"execution_count":null,"outputs":[]}]}